{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68630a17-4a94-4d6f-b994-86120fc7a77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Obtaining dependency information for lightgbm from https://files.pythonhosted.org/packages/ba/11/cb8b67f3cbdca05b59a032bb57963d4fe8c8d18c3870f30bed005b7f174d/lightgbm-4.3.0-py3-none-manylinux_2_28_x86_64.whl.metadata\n",
      "  Using cached lightgbm-4.3.0-py3-none-manylinux_2_28_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from lightgbm) (1.23.5)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from lightgbm) (1.11.2)\n",
      "Using cached lightgbm-4.3.0-py3-none-manylinux_2_28_x86_64.whl (3.1 MB)\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.3.0\n",
      "Collecting pytorch_tabnet\n",
      "  Obtaining dependency information for pytorch_tabnet from https://files.pythonhosted.org/packages/0f/92/ed98b89b7cf5661656daa4cc88e578f712eb5eae41b8f46a56c1ece3a895/pytorch_tabnet-4.1.0-py3-none-any.whl.metadata\n",
      "  Using cached pytorch_tabnet-4.1.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from pytorch_tabnet) (1.23.5)\n",
      "Requirement already satisfied: scikit_learn>0.21 in /opt/conda/lib/python3.10/site-packages (from pytorch_tabnet) (1.3.1)\n",
      "Requirement already satisfied: scipy>1.4 in /opt/conda/lib/python3.10/site-packages (from pytorch_tabnet) (1.11.2)\n",
      "Collecting torch>=1.3 (from pytorch_tabnet)\n",
      "  Obtaining dependency information for torch>=1.3 from https://files.pythonhosted.org/packages/03/f1/13137340776dd5d5bcfd2574c9c6dfcc7618285035cd77240496e5c1a79b/torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl.metadata\n",
      "  Using cached torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: tqdm>=4.36 in /opt/conda/lib/python3.10/site-packages (from pytorch_tabnet) (4.66.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit_learn>0.21->pytorch_tabnet) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit_learn>0.21->pytorch_tabnet) (3.2.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.3->pytorch_tabnet) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.3->pytorch_tabnet) (4.8.0)\n",
      "Collecting sympy (from torch>=1.3->pytorch_tabnet)\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.3->pytorch_tabnet) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.3->pytorch_tabnet) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.3->pytorch_tabnet) (2023.9.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.3->pytorch_tabnet)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.3->pytorch_tabnet)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.3->pytorch_tabnet)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.3->pytorch_tabnet)\n",
      "  Obtaining dependency information for nvidia-cudnn-cu12==8.9.2.26 from https://files.pythonhosted.org/packages/ff/74/a2e2be7fb83aaedec84f391f082cf765dfb635e7caa9b49065f73e4835d8/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.3->pytorch_tabnet)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.3->pytorch_tabnet)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.3->pytorch_tabnet)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.3->pytorch_tabnet)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.3->pytorch_tabnet)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-nccl-cu12==2.18.1 (from torch>=1.3->pytorch_tabnet)\n",
      "  Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.3->pytorch_tabnet)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Collecting triton==2.1.0 (from torch>=1.3->pytorch_tabnet)\n",
      "  Obtaining dependency information for triton==2.1.0 from https://files.pythonhosted.org/packages/4d/22/91a8af421c8a8902dde76e6ef3db01b258af16c53d81e8c0d0dc13900a9e/triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Using cached triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.3->pytorch_tabnet)\n",
      "  Obtaining dependency information for nvidia-nvjitlink-cu12 from https://files.pythonhosted.org/packages/1e/07/bf730d44c2fe1b676ad9cc2be5f5f861eb5d153fb6951987a2d6a96379a9/nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Using cached nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.3->pytorch_tabnet) (2.1.3)\n",
      "Collecting mpmath>=0.19 (from sympy->torch>=1.3->pytorch_tabnet)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n",
      "Using cached torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Using cached triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
      "Installing collected packages: mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, pytorch_tabnet\n",
      "Successfully installed mpmath-1.3.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 pytorch_tabnet-4.1.0 sympy-1.12 torch-2.1.2 triton-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm\n",
    "!pip install pytorch_tabnet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6414d704-f5a7-42ae-a6e8-d9fddc724765",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "new_index = range(42307, 42307 + len(test))\n",
    "test.index = new_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07329754-8bf2-4a11-b9ee-95f293169f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42307 entries, 0 to 42306\n",
      "Data columns (total 20 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Term               42307 non-null  int64  \n",
      " 1   NoEmp              42307 non-null  int64  \n",
      " 2   NewExist           42307 non-null  float64\n",
      " 3   CreateJob          42307 non-null  int64  \n",
      " 4   RetainedJob        42307 non-null  int64  \n",
      " 5   FranchiseCode      42307 non-null  int64  \n",
      " 6   RevLineCr          41228 non-null  object \n",
      " 7   LowDoc             41776 non-null  object \n",
      " 8   DisbursementDate   42157 non-null  object \n",
      " 9   MIS_Status         42307 non-null  int64  \n",
      " 10  Sector             42307 non-null  int64  \n",
      " 11  ApprovalDate       42307 non-null  object \n",
      " 12  ApprovalFY         42307 non-null  int64  \n",
      " 13  City               42307 non-null  object \n",
      " 14  State              42307 non-null  object \n",
      " 15  BankState          42296 non-null  object \n",
      " 16  DisbursementGross  42307 non-null  object \n",
      " 17  GrAppv             42307 non-null  object \n",
      " 18  SBA_Appv           42307 non-null  object \n",
      " 19  UrbanRural         42307 non-null  int64  \n",
      "dtypes: float64(1), int64(9), object(10)\n",
      "memory usage: 6.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "513f20bd-6142-4f4b-89a1-f8e02f8312fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, replace_dict=None, ce_dict=None):\n",
    "\n",
    "    # 借り手の会社に関する変数（Sector, FranchiseCode）\n",
    "    # 31-33, 44-45, 48-49 は同じらしい => 32,33を31に, 45を44に, 49を48に変換\n",
    "    code_dict = {\n",
    "        32: 31,\n",
    "        33: 31,\n",
    "        45: 44,\n",
    "        49: 48\n",
    "    }\n",
    "    df[\"Sector\"] = df[\"Sector\"].replace(code_dict)\n",
    "\n",
    "    # 今回の借り入れに関する変数（RevLineCr, LowDoc）\n",
    "    # 公式ページには値の候補が2つ（YesとNoのYN）と記載があるが、実際の値の種類は2より多い。YN以外はNaNへ置換\n",
    "    revline_dict = {'0': np.nan, 'T': np.nan}\n",
    "    df[\"RevLineCr\"] = df[\"RevLineCr\"].replace(revline_dict)\n",
    "\n",
    "    lowdoc_dict = {'C': np.nan, '0': np.nan, 'S': np.nan, 'A': np.nan}\n",
    "    df[\"LowDoc\"] = df[\"LowDoc\"].replace(lowdoc_dict)\n",
    "\n",
    "    # 日付系の変数（DisbursementDate, ApprovalDate）\n",
    "    # 日付型へ変更 → 年を抽出（借りた月や日にはあまり意味はないと思われるため）\n",
    "    df['DisbursementDate'] = pd.to_datetime(df['DisbursementDate'], format='%d-%b-%y')\n",
    "    df[\"DisbursementYear\"] = df[\"DisbursementDate\"].dt.year\n",
    "\n",
    "    # 本来数値型のものを変換する\n",
    "    cols = [\"DisbursementGross\", \"GrAppv\", \"SBA_Appv\"]\n",
    "    df[cols] = df[cols].applymap(lambda x: x.strip().replace('$', '').replace(',', '')).astype(float).astype(int)\n",
    "\n",
    "    # 特徴量エンジニアリング\n",
    "    df[\"FY_Diff\"] = df[\"ApprovalFY\"] - df[\"DisbursementYear\"]\n",
    "    df[\"State_is_BankState\"] = (df[\"State\"] == df[\"BankState\"])\n",
    "    df[\"State_is_BankState\"] = df[\"State_is_BankState\"].replace({True: 1, False: 0})\n",
    "\n",
    "    df['SBA_Portion'] = df['SBA_Appv'] / df['GrAppv']\n",
    "    df[\"DisbursementGrossRatio\"] = df[\"DisbursementGross\"] / df[\"GrAppv\"]\n",
    "    df[\"MonthlyRepayment\"] = df[\"GrAppv\"] / df[\"Term\"]\n",
    "    df[\"NullCount\"] = df.isnull().sum(axis=1)\n",
    "\n",
    "    # カテゴリカル変数の設定\n",
    "    df[cols_category] = df[cols_category].fillna(-1)\n",
    "\n",
    "    # train\n",
    "    if replace_dict is None:\n",
    "        # countencode, labelencode\n",
    "        # ce_dict: 列名を入れるとそのカテゴリのデータがどのくらいあるかを返してくれます\n",
    "        # replace_dict: 列名を入れるとlabelencodeのための数字を返してくれます\n",
    "        ce_dict = {}\n",
    "        replace_dict = {}\n",
    "        for col in cols_category:\n",
    "            replace_dict[col] = {}\n",
    "            vc = df[col].value_counts()\n",
    "            ce_dict[col] = vc\n",
    "            replace_dict_in_dict = {}\n",
    "            for i, k in enumerate(vc.keys()):\n",
    "                replace_dict_in_dict[k] = i\n",
    "            replace_dict[col] = replace_dict_in_dict\n",
    "            df[f\"{col}_CountEncode\"] = df[col].replace(vc).astype(int)\n",
    "            df[col] = df[col].replace(replace_dict_in_dict).astype(int)\n",
    "        return df, replace_dict, ce_dict\n",
    "\n",
    "    # test\n",
    "    else:\n",
    "        for col in cols_category:\n",
    "            # カウントエンコード\n",
    "            test_vals_uniq = df[col].unique()\n",
    "            ce_dict_in_dict = ce_dict[col]\n",
    "            for test_val in test_vals_uniq:\n",
    "                if test_val not in ce_dict_in_dict.keys():\n",
    "                    ce_dict_in_dict[test_val] = -1\n",
    "            df[f\"{col}_CountEncode\"] = df[col].replace(ce_dict_in_dict).astype(int)\n",
    "\n",
    "            # LabelEncode\n",
    "            test_vals_uniq = df[col].unique()\n",
    "            replace_dict_in_dict = replace_dict[col]\n",
    "            for test_val in test_vals_uniq:\n",
    "                if test_val not in replace_dict_in_dict.keys():\n",
    "                    replace_dict_in_dict[test_val] = -1\n",
    "            df[col] = df[col].replace(replace_dict_in_dict).astype(int)\n",
    "        return df\n",
    "    df.drop([\"DisbursementDate\", \"ApprovalDate\",\"City\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1479424e-6843-42bf-8b7d-73f34341bc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_category = []\n",
    "for col in df.columns:\n",
    "    if df[col].dtypes == 'object':\n",
    "        cols_category.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69422f02-269d-442a-b743-c15427353a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, replace_dict, ce_dict = preprocess(df)\n",
    "\n",
    "test_df = preprocess(test, replace_dict, ce_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "794d6696-e6c2-4e84-bc27-2ad340a8ae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.metrics import Metric\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "class F1Metric(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"f1\"\n",
    "        self._maximize = True\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        y_pred = np.argmax(y_score, axis=1)\n",
    "        return f1_score(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "959d2bd8-253b-41cb-90ca-c369f7f03238",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace([np.inf, -np.inf], np.nan) \n",
    "df_dropna = df.dropna(axis=0)\n",
    "X = df_dropna.drop('MIS_Status',axis=1)\n",
    "y = df_dropna['MIS_Status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df995a93-58db-4fa0-8dde-e3cf5ba0f668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "無限大の値を含む行:\n",
      "Empty DataFrame\n",
      "Columns: [Term, NoEmp, NewExist, CreateJob, RetainedJob, FranchiseCode, RevLineCr, LowDoc, DisbursementDate, Sector, ApprovalDate, ApprovalFY, City, State, BankState, DisbursementGross, GrAppv, SBA_Appv, UrbanRural, DisbursementYear, FY_Diff, State_is_BankState, SBA_Portion, DisbursementGrossRatio, MonthlyRepayment, NullCount, RevLineCr_CountEncode, LowDoc_CountEncode, DisbursementDate_CountEncode, ApprovalDate_CountEncode, City_CountEncode, State_CountEncode, BankState_CountEncode, DisbursementGross_CountEncode, GrAppv_CountEncode, SBA_Appv_CountEncode]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 36 columns]\n",
      "無限大の値を含む列:\n",
      "Term                             False\n",
      "NoEmp                            False\n",
      "NewExist                         False\n",
      "CreateJob                        False\n",
      "RetainedJob                      False\n",
      "FranchiseCode                    False\n",
      "RevLineCr                        False\n",
      "LowDoc                           False\n",
      "DisbursementDate                 False\n",
      "MIS_Status                       False\n",
      "Sector                           False\n",
      "ApprovalDate                     False\n",
      "ApprovalFY                       False\n",
      "City                             False\n",
      "State                            False\n",
      "BankState                        False\n",
      "DisbursementGross                False\n",
      "GrAppv                           False\n",
      "SBA_Appv                         False\n",
      "UrbanRural                       False\n",
      "DisbursementYear                 False\n",
      "FY_Diff                          False\n",
      "State_is_BankState               False\n",
      "SBA_Portion                      False\n",
      "DisbursementGrossRatio           False\n",
      "MonthlyRepayment                 False\n",
      "NullCount                        False\n",
      "RevLineCr_CountEncode            False\n",
      "LowDoc_CountEncode               False\n",
      "DisbursementDate_CountEncode     False\n",
      "ApprovalDate_CountEncode         False\n",
      "City_CountEncode                 False\n",
      "State_CountEncode                False\n",
      "BankState_CountEncode            False\n",
      "DisbursementGross_CountEncode    False\n",
      "GrAppv_CountEncode               False\n",
      "SBA_Appv_CountEncode             False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "inf_rows = df_dropna.isin([np.inf, -np.inf]).any(axis=1)\n",
    "inf_cols = df_dropna.isin([np.inf, -np.inf]).any(axis=0)\n",
    "\n",
    "print(\"無限大の値を含む行:\")\n",
    "print(X[inf_rows])\n",
    "\n",
    "print(\"無限大の値を含む列:\")\n",
    "print(inf_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "844b06bc-9d25-4b07-a307-e41eb1527cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X.replace([np.inf, -np.inf], np.mean, inplace=True)\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "881f69c0-6ea9-4a9e-8d83-07a5a13fa776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "weights = class_weight.compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "class_weights = dict(zip(np.unique(y), weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c9c80291-12b2-42d0-a016-5b771c4471dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.64009 | val_0_auc: 0.70667 |  0:00:03s\n",
      "epoch 1  | loss: 0.6048  | val_0_auc: 0.73119 |  0:00:07s\n",
      "epoch 2  | loss: 0.58924 | val_0_auc: 0.74313 |  0:00:11s\n",
      "epoch 3  | loss: 0.58406 | val_0_auc: 0.75015 |  0:00:15s\n",
      "epoch 4  | loss: 0.57859 | val_0_auc: 0.75152 |  0:00:19s\n",
      "epoch 5  | loss: 0.57929 | val_0_auc: 0.75835 |  0:00:23s\n",
      "epoch 6  | loss: 0.57322 | val_0_auc: 0.75443 |  0:00:26s\n",
      "epoch 7  | loss: 0.56774 | val_0_auc: 0.76175 |  0:00:30s\n",
      "epoch 8  | loss: 0.57345 | val_0_auc: 0.75657 |  0:00:34s\n",
      "epoch 9  | loss: 0.55865 | val_0_auc: 0.76133 |  0:00:38s\n",
      "epoch 10 | loss: 0.56258 | val_0_auc: 0.7611  |  0:00:42s\n",
      "epoch 11 | loss: 0.55672 | val_0_auc: 0.75037 |  0:00:46s\n",
      "epoch 12 | loss: 0.56239 | val_0_auc: 0.7564  |  0:00:50s\n",
      "epoch 13 | loss: 0.55343 | val_0_auc: 0.76181 |  0:00:53s\n",
      "epoch 14 | loss: 0.54918 | val_0_auc: 0.76098 |  0:00:57s\n",
      "epoch 15 | loss: 0.54994 | val_0_auc: 0.76145 |  0:01:01s\n",
      "epoch 16 | loss: 0.54265 | val_0_auc: 0.76087 |  0:01:05s\n",
      "epoch 17 | loss: 0.54399 | val_0_auc: 0.76157 |  0:01:09s\n",
      "epoch 18 | loss: 0.54192 | val_0_auc: 0.75807 |  0:01:12s\n",
      "epoch 19 | loss: 0.53455 | val_0_auc: 0.75784 |  0:01:16s\n",
      "epoch 20 | loss: 0.53896 | val_0_auc: 0.75773 |  0:01:20s\n",
      "epoch 21 | loss: 0.53182 | val_0_auc: 0.75033 |  0:01:24s\n",
      "epoch 22 | loss: 0.53273 | val_0_auc: 0.7552  |  0:01:28s\n",
      "epoch 23 | loss: 0.52982 | val_0_auc: 0.75094 |  0:01:32s\n",
      "epoch 24 | loss: 0.5281  | val_0_auc: 0.74928 |  0:01:35s\n",
      "epoch 25 | loss: 0.5246  | val_0_auc: 0.75025 |  0:01:39s\n",
      "epoch 26 | loss: 0.51903 | val_0_auc: 0.74418 |  0:01:43s\n",
      "epoch 27 | loss: 0.52163 | val_0_auc: 0.74849 |  0:01:47s\n",
      "epoch 28 | loss: 0.5209  | val_0_auc: 0.74507 |  0:01:51s\n",
      "epoch 29 | loss: 0.51394 | val_0_auc: 0.73982 |  0:01:55s\n",
      "epoch 30 | loss: 0.50517 | val_0_auc: 0.74487 |  0:01:58s\n",
      "epoch 31 | loss: 0.50637 | val_0_auc: 0.75334 |  0:02:02s\n",
      "epoch 32 | loss: 0.50531 | val_0_auc: 0.73825 |  0:02:06s\n",
      "epoch 33 | loss: 0.50218 | val_0_auc: 0.74938 |  0:02:10s\n",
      "epoch 34 | loss: 0.49438 | val_0_auc: 0.75133 |  0:02:14s\n",
      "epoch 35 | loss: 0.50172 | val_0_auc: 0.74543 |  0:02:18s\n",
      "epoch 36 | loss: 0.49779 | val_0_auc: 0.74097 |  0:02:21s\n",
      "epoch 37 | loss: 0.49561 | val_0_auc: 0.7418  |  0:02:25s\n",
      "epoch 38 | loss: 0.49258 | val_0_auc: 0.74069 |  0:02:29s\n",
      "epoch 39 | loss: 0.49591 | val_0_auc: 0.74605 |  0:02:33s\n",
      "epoch 40 | loss: 0.48797 | val_0_auc: 0.73259 |  0:02:37s\n",
      "epoch 41 | loss: 0.49099 | val_0_auc: 0.73978 |  0:02:41s\n",
      "epoch 42 | loss: 0.49183 | val_0_auc: 0.74016 |  0:02:45s\n",
      "epoch 43 | loss: 0.4859  | val_0_auc: 0.74178 |  0:02:48s\n",
      "epoch 44 | loss: 0.48576 | val_0_auc: 0.73443 |  0:02:52s\n",
      "epoch 45 | loss: 0.47642 | val_0_auc: 0.73642 |  0:02:56s\n",
      "epoch 46 | loss: 0.48014 | val_0_auc: 0.73897 |  0:03:00s\n",
      "epoch 47 | loss: 0.47532 | val_0_auc: 0.7389  |  0:03:04s\n",
      "epoch 48 | loss: 0.48079 | val_0_auc: 0.73276 |  0:03:08s\n",
      "epoch 49 | loss: 0.47573 | val_0_auc: 0.73856 |  0:03:11s\n",
      "Stop training because you reached max_epochs = 50 with best_epoch = 13 and best_val_0_auc = 0.76181\n",
      "    feature  importance\n",
      "18       18    0.123375\n",
      "28       28    0.110088\n",
      "33       33    0.107297\n",
      "0         0    0.092609\n",
      "1         1    0.082793\n",
      "7         7    0.060514\n",
      "3         3    0.058549\n",
      "12       12    0.055144\n",
      "25       25    0.052832\n",
      "2         2    0.050090\n",
      "4         4    0.035110\n",
      "6         6    0.025946\n",
      "27       27    0.023871\n",
      "21       21    0.020358\n",
      "23       23    0.019402\n",
      "26       26    0.018784\n",
      "14       14    0.017377\n",
      "5         5    0.013473\n",
      "11       11    0.012662\n",
      "32       32    0.007172\n",
      "30       30    0.004495\n",
      "16       16    0.001890\n",
      "35       35    0.001508\n",
      "34       34    0.001356\n",
      "20       20    0.001117\n",
      "22       22    0.001071\n",
      "8         8    0.000546\n",
      "24       24    0.000203\n",
      "17       17    0.000153\n",
      "31       31    0.000149\n",
      "15       15    0.000049\n",
      "13       13    0.000010\n",
      "19       19    0.000004\n",
      "29       29    0.000003\n",
      "10       10    0.000001\n",
      "9         9    0.000001\n",
      "Mean F1 Score (Macro F1 Score): 0.5909045435660534\n",
      "epoch 0  | loss: 0.64095 | val_0_auc: 0.72708 |  0:00:03s\n",
      "epoch 1  | loss: 0.59772 | val_0_auc: 0.74261 |  0:00:07s\n",
      "epoch 2  | loss: 0.58559 | val_0_auc: 0.74793 |  0:00:11s\n",
      "epoch 3  | loss: 0.58233 | val_0_auc: 0.74064 |  0:00:15s\n",
      "epoch 4  | loss: 0.57719 | val_0_auc: 0.74468 |  0:00:19s\n",
      "epoch 5  | loss: 0.57622 | val_0_auc: 0.74667 |  0:00:23s\n",
      "epoch 6  | loss: 0.57462 | val_0_auc: 0.75179 |  0:00:27s\n",
      "epoch 7  | loss: 0.56928 | val_0_auc: 0.75658 |  0:00:32s\n",
      "epoch 8  | loss: 0.57053 | val_0_auc: 0.75175 |  0:00:36s\n",
      "epoch 9  | loss: 0.55546 | val_0_auc: 0.75492 |  0:00:39s\n",
      "epoch 10 | loss: 0.55377 | val_0_auc: 0.75238 |  0:00:43s\n",
      "epoch 11 | loss: 0.54926 | val_0_auc: 0.74909 |  0:00:47s\n",
      "epoch 12 | loss: 0.55411 | val_0_auc: 0.74342 |  0:00:51s\n",
      "epoch 13 | loss: 0.54725 | val_0_auc: 0.7487  |  0:00:55s\n",
      "epoch 14 | loss: 0.53673 | val_0_auc: 0.74697 |  0:00:59s\n",
      "epoch 15 | loss: 0.54011 | val_0_auc: 0.75215 |  0:01:03s\n",
      "epoch 16 | loss: 0.53771 | val_0_auc: 0.73959 |  0:01:07s\n",
      "epoch 17 | loss: 0.53066 | val_0_auc: 0.73999 |  0:01:11s\n",
      "epoch 18 | loss: 0.52795 | val_0_auc: 0.74071 |  0:01:14s\n",
      "epoch 19 | loss: 0.52174 | val_0_auc: 0.74202 |  0:01:18s\n",
      "epoch 20 | loss: 0.52167 | val_0_auc: 0.73275 |  0:01:22s\n",
      "epoch 21 | loss: 0.51464 | val_0_auc: 0.73908 |  0:01:26s\n",
      "epoch 22 | loss: 0.51356 | val_0_auc: 0.73847 |  0:01:30s\n",
      "epoch 23 | loss: 0.51556 | val_0_auc: 0.73775 |  0:01:34s\n",
      "epoch 24 | loss: 0.51116 | val_0_auc: 0.73883 |  0:01:38s\n",
      "epoch 25 | loss: 0.51256 | val_0_auc: 0.73892 |  0:01:41s\n",
      "epoch 26 | loss: 0.50939 | val_0_auc: 0.74204 |  0:01:45s\n",
      "epoch 27 | loss: 0.5061  | val_0_auc: 0.73196 |  0:01:49s\n",
      "epoch 28 | loss: 0.50821 | val_0_auc: 0.73007 |  0:01:53s\n",
      "epoch 29 | loss: 0.49991 | val_0_auc: 0.72895 |  0:01:57s\n",
      "epoch 30 | loss: 0.49681 | val_0_auc: 0.72071 |  0:02:01s\n",
      "epoch 31 | loss: 0.49497 | val_0_auc: 0.73626 |  0:02:04s\n",
      "epoch 32 | loss: 0.49436 | val_0_auc: 0.72799 |  0:02:08s\n",
      "epoch 33 | loss: 0.49651 | val_0_auc: 0.7358  |  0:02:12s\n",
      "epoch 34 | loss: 0.50461 | val_0_auc: 0.73255 |  0:02:16s\n",
      "epoch 35 | loss: 0.49322 | val_0_auc: 0.7298  |  0:02:20s\n",
      "epoch 36 | loss: 0.48817 | val_0_auc: 0.73904 |  0:02:24s\n",
      "epoch 37 | loss: 0.48691 | val_0_auc: 0.72663 |  0:02:28s\n",
      "epoch 38 | loss: 0.48751 | val_0_auc: 0.73153 |  0:02:32s\n",
      "epoch 39 | loss: 0.4816  | val_0_auc: 0.73281 |  0:02:36s\n",
      "epoch 40 | loss: 0.4886  | val_0_auc: 0.72762 |  0:02:40s\n",
      "epoch 41 | loss: 0.47788 | val_0_auc: 0.72525 |  0:02:44s\n",
      "epoch 42 | loss: 0.48024 | val_0_auc: 0.72832 |  0:02:47s\n",
      "epoch 43 | loss: 0.47566 | val_0_auc: 0.72539 |  0:02:51s\n",
      "epoch 44 | loss: 0.47654 | val_0_auc: 0.72914 |  0:02:55s\n",
      "epoch 45 | loss: 0.47293 | val_0_auc: 0.72387 |  0:02:59s\n",
      "epoch 46 | loss: 0.47124 | val_0_auc: 0.72505 |  0:03:03s\n",
      "epoch 47 | loss: 0.46849 | val_0_auc: 0.72298 |  0:03:07s\n",
      "epoch 48 | loss: 0.46665 | val_0_auc: 0.72719 |  0:03:10s\n",
      "epoch 49 | loss: 0.47068 | val_0_auc: 0.72725 |  0:03:14s\n",
      "Stop training because you reached max_epochs = 50 with best_epoch = 7 and best_val_0_auc = 0.75658\n",
      "    feature  importance\n",
      "18       18    0.280381\n",
      "1         1    0.141565\n",
      "2         2    0.087196\n",
      "25       25    0.078108\n",
      "28       28    0.059598\n",
      "34       34    0.048405\n",
      "21       21    0.038002\n",
      "19       19    0.033376\n",
      "23       23    0.029060\n",
      "26       26    0.028281\n",
      "7         7    0.026253\n",
      "0         0    0.026027\n",
      "33       33    0.018852\n",
      "30       30    0.014978\n",
      "5         5    0.012803\n",
      "22       22    0.012373\n",
      "35       35    0.010429\n",
      "4         4    0.009848\n",
      "12       12    0.007681\n",
      "32       32    0.006984\n",
      "11       11    0.006920\n",
      "9         9    0.005615\n",
      "17       17    0.004875\n",
      "3         3    0.004802\n",
      "29       29    0.002280\n",
      "13       13    0.001735\n",
      "27       27    0.001508\n",
      "10       10    0.001377\n",
      "6         6    0.000231\n",
      "24       24    0.000160\n",
      "16       16    0.000080\n",
      "31       31    0.000080\n",
      "14       14    0.000063\n",
      "8         8    0.000054\n",
      "20       20    0.000021\n",
      "15       15    0.000001\n",
      "Mean F1 Score (Macro F1 Score): 0.5695541079669616\n",
      "epoch 0  | loss: 0.63081 | val_0_auc: 0.72823 |  0:00:03s\n",
      "epoch 1  | loss: 0.59055 | val_0_auc: 0.73851 |  0:00:07s\n",
      "epoch 2  | loss: 0.58341 | val_0_auc: 0.73972 |  0:00:12s\n",
      "epoch 3  | loss: 0.58168 | val_0_auc: 0.74879 |  0:00:16s\n",
      "epoch 4  | loss: 0.57802 | val_0_auc: 0.75285 |  0:00:19s\n",
      "epoch 5  | loss: 0.57484 | val_0_auc: 0.74494 |  0:00:24s\n",
      "epoch 6  | loss: 0.56597 | val_0_auc: 0.74554 |  0:00:28s\n",
      "epoch 7  | loss: 0.5615  | val_0_auc: 0.74777 |  0:00:32s\n",
      "epoch 8  | loss: 0.56521 | val_0_auc: 0.74945 |  0:00:36s\n",
      "epoch 9  | loss: 0.55823 | val_0_auc: 0.74735 |  0:00:40s\n",
      "epoch 10 | loss: 0.55228 | val_0_auc: 0.74604 |  0:00:43s\n",
      "epoch 11 | loss: 0.54975 | val_0_auc: 0.74294 |  0:00:47s\n",
      "epoch 12 | loss: 0.54881 | val_0_auc: 0.74616 |  0:00:51s\n",
      "epoch 13 | loss: 0.54154 | val_0_auc: 0.74713 |  0:00:55s\n",
      "epoch 14 | loss: 0.54004 | val_0_auc: 0.74863 |  0:00:59s\n",
      "epoch 15 | loss: 0.54678 | val_0_auc: 0.75059 |  0:01:03s\n",
      "epoch 16 | loss: 0.53963 | val_0_auc: 0.74209 |  0:01:07s\n",
      "epoch 17 | loss: 0.53911 | val_0_auc: 0.73584 |  0:01:11s\n",
      "epoch 18 | loss: 0.52893 | val_0_auc: 0.73886 |  0:01:14s\n",
      "epoch 19 | loss: 0.53057 | val_0_auc: 0.7429  |  0:01:18s\n",
      "epoch 20 | loss: 0.52675 | val_0_auc: 0.73926 |  0:01:22s\n",
      "epoch 21 | loss: 0.52476 | val_0_auc: 0.73276 |  0:01:26s\n",
      "epoch 22 | loss: 0.51701 | val_0_auc: 0.74117 |  0:01:30s\n",
      "epoch 23 | loss: 0.51336 | val_0_auc: 0.73413 |  0:01:34s\n",
      "epoch 24 | loss: 0.513   | val_0_auc: 0.74025 |  0:01:38s\n",
      "epoch 25 | loss: 0.51354 | val_0_auc: 0.73818 |  0:01:42s\n",
      "epoch 26 | loss: 0.51059 | val_0_auc: 0.73854 |  0:01:46s\n",
      "epoch 27 | loss: 0.50934 | val_0_auc: 0.73818 |  0:01:50s\n",
      "epoch 28 | loss: 0.50448 | val_0_auc: 0.73889 |  0:01:54s\n",
      "epoch 29 | loss: 0.50133 | val_0_auc: 0.72435 |  0:01:58s\n",
      "epoch 30 | loss: 0.496   | val_0_auc: 0.73662 |  0:02:02s\n",
      "epoch 31 | loss: 0.49633 | val_0_auc: 0.72354 |  0:02:06s\n",
      "epoch 32 | loss: 0.49345 | val_0_auc: 0.72938 |  0:02:10s\n",
      "epoch 33 | loss: 0.49395 | val_0_auc: 0.73836 |  0:02:14s\n",
      "epoch 34 | loss: 0.49006 | val_0_auc: 0.73241 |  0:02:17s\n",
      "epoch 35 | loss: 0.48922 | val_0_auc: 0.72957 |  0:02:21s\n",
      "epoch 36 | loss: 0.49038 | val_0_auc: 0.72288 |  0:02:25s\n",
      "epoch 37 | loss: 0.48798 | val_0_auc: 0.72562 |  0:02:29s\n",
      "epoch 38 | loss: 0.48906 | val_0_auc: 0.72302 |  0:02:34s\n",
      "epoch 39 | loss: 0.48747 | val_0_auc: 0.72881 |  0:02:38s\n",
      "epoch 40 | loss: 0.48066 | val_0_auc: 0.72079 |  0:02:42s\n",
      "epoch 41 | loss: 0.47645 | val_0_auc: 0.72423 |  0:02:46s\n",
      "epoch 42 | loss: 0.47873 | val_0_auc: 0.72337 |  0:02:49s\n",
      "epoch 43 | loss: 0.47888 | val_0_auc: 0.72115 |  0:02:53s\n",
      "epoch 44 | loss: 0.47504 | val_0_auc: 0.72237 |  0:02:57s\n",
      "epoch 45 | loss: 0.46922 | val_0_auc: 0.72625 |  0:03:01s\n",
      "epoch 46 | loss: 0.47149 | val_0_auc: 0.72248 |  0:03:05s\n",
      "epoch 47 | loss: 0.4635  | val_0_auc: 0.72472 |  0:03:09s\n",
      "epoch 48 | loss: 0.46108 | val_0_auc: 0.71124 |  0:03:13s\n",
      "epoch 49 | loss: 0.46757 | val_0_auc: 0.7218  |  0:03:17s\n",
      "Stop training because you reached max_epochs = 50 with best_epoch = 4 and best_val_0_auc = 0.75285\n",
      "    feature  importance\n",
      "18       18    0.156283\n",
      "1         1    0.155507\n",
      "25       25    0.096611\n",
      "9         9    0.079971\n",
      "29       29    0.078128\n",
      "26       26    0.067957\n",
      "0         0    0.064946\n",
      "2         2    0.025139\n",
      "7         7    0.023942\n",
      "20       20    0.023259\n",
      "27       27    0.023149\n",
      "3         3    0.021508\n",
      "30       30    0.021091\n",
      "8         8    0.019944\n",
      "17       17    0.016720\n",
      "6         6    0.015209\n",
      "10       10    0.014855\n",
      "13       13    0.013996\n",
      "28       28    0.012437\n",
      "11       11    0.012189\n",
      "12       12    0.009458\n",
      "31       31    0.008884\n",
      "4         4    0.006130\n",
      "5         5    0.005701\n",
      "19       19    0.005611\n",
      "35       35    0.005520\n",
      "34       34    0.004609\n",
      "32       32    0.004512\n",
      "21       21    0.002523\n",
      "15       15    0.001594\n",
      "23       23    0.000850\n",
      "16       16    0.000768\n",
      "33       33    0.000607\n",
      "14       14    0.000224\n",
      "24       24    0.000153\n",
      "22       22    0.000016\n",
      "Mean F1 Score (Macro F1 Score): 0.5830899542121528\n",
      "epoch 0  | loss: 0.64563 | val_0_auc: 0.71687 |  0:00:03s\n",
      "epoch 1  | loss: 0.59606 | val_0_auc: 0.74525 |  0:00:07s\n",
      "epoch 2  | loss: 0.58407 | val_0_auc: 0.75392 |  0:00:11s\n",
      "epoch 3  | loss: 0.58508 | val_0_auc: 0.75311 |  0:00:15s\n",
      "epoch 4  | loss: 0.57376 | val_0_auc: 0.75784 |  0:00:19s\n",
      "epoch 5  | loss: 0.56832 | val_0_auc: 0.76071 |  0:00:23s\n",
      "epoch 6  | loss: 0.5657  | val_0_auc: 0.75695 |  0:00:27s\n",
      "epoch 7  | loss: 0.56209 | val_0_auc: 0.75006 |  0:00:31s\n",
      "epoch 8  | loss: 0.56109 | val_0_auc: 0.75646 |  0:00:34s\n",
      "epoch 9  | loss: 0.5573  | val_0_auc: 0.75335 |  0:00:38s\n",
      "epoch 10 | loss: 0.55368 | val_0_auc: 0.75442 |  0:00:42s\n",
      "epoch 11 | loss: 0.55694 | val_0_auc: 0.75101 |  0:00:46s\n",
      "epoch 12 | loss: 0.55076 | val_0_auc: 0.75472 |  0:00:50s\n",
      "epoch 13 | loss: 0.54796 | val_0_auc: 0.7554  |  0:00:54s\n",
      "epoch 14 | loss: 0.54455 | val_0_auc: 0.75226 |  0:00:58s\n",
      "epoch 15 | loss: 0.54011 | val_0_auc: 0.75027 |  0:01:02s\n",
      "epoch 16 | loss: 0.54341 | val_0_auc: 0.75104 |  0:01:06s\n",
      "epoch 17 | loss: 0.54015 | val_0_auc: 0.74464 |  0:01:10s\n",
      "epoch 18 | loss: 0.53623 | val_0_auc: 0.75471 |  0:01:13s\n",
      "epoch 19 | loss: 0.52946 | val_0_auc: 0.74782 |  0:01:17s\n",
      "epoch 20 | loss: 0.53355 | val_0_auc: 0.74491 |  0:01:21s\n",
      "epoch 21 | loss: 0.52862 | val_0_auc: 0.74822 |  0:01:25s\n",
      "epoch 22 | loss: 0.52206 | val_0_auc: 0.74669 |  0:01:29s\n",
      "epoch 23 | loss: 0.51948 | val_0_auc: 0.7513  |  0:01:34s\n",
      "epoch 24 | loss: 0.51848 | val_0_auc: 0.74677 |  0:01:38s\n",
      "epoch 25 | loss: 0.51923 | val_0_auc: 0.74101 |  0:01:41s\n",
      "epoch 26 | loss: 0.52536 | val_0_auc: 0.74104 |  0:01:45s\n",
      "epoch 27 | loss: 0.51947 | val_0_auc: 0.74524 |  0:01:49s\n",
      "epoch 28 | loss: 0.51374 | val_0_auc: 0.74783 |  0:01:53s\n",
      "epoch 29 | loss: 0.50561 | val_0_auc: 0.7394  |  0:01:57s\n",
      "epoch 30 | loss: 0.4991  | val_0_auc: 0.7433  |  0:02:01s\n",
      "epoch 31 | loss: 0.50425 | val_0_auc: 0.73935 |  0:02:05s\n",
      "epoch 32 | loss: 0.50077 | val_0_auc: 0.74191 |  0:02:09s\n",
      "epoch 33 | loss: 0.49895 | val_0_auc: 0.72928 |  0:02:13s\n",
      "epoch 34 | loss: 0.49721 | val_0_auc: 0.74426 |  0:02:17s\n",
      "epoch 35 | loss: 0.49446 | val_0_auc: 0.74253 |  0:02:21s\n",
      "epoch 36 | loss: 0.49417 | val_0_auc: 0.73351 |  0:02:25s\n",
      "epoch 37 | loss: 0.49116 | val_0_auc: 0.74006 |  0:02:29s\n",
      "epoch 38 | loss: 0.48257 | val_0_auc: 0.74252 |  0:02:33s\n",
      "epoch 39 | loss: 0.4819  | val_0_auc: 0.73754 |  0:02:37s\n",
      "epoch 40 | loss: 0.48752 | val_0_auc: 0.74218 |  0:02:41s\n",
      "epoch 41 | loss: 0.48481 | val_0_auc: 0.73359 |  0:02:45s\n",
      "epoch 42 | loss: 0.47734 | val_0_auc: 0.73962 |  0:02:49s\n",
      "epoch 43 | loss: 0.47637 | val_0_auc: 0.73459 |  0:02:53s\n",
      "epoch 44 | loss: 0.482   | val_0_auc: 0.73681 |  0:02:57s\n",
      "epoch 45 | loss: 0.47641 | val_0_auc: 0.7264  |  0:03:01s\n",
      "epoch 46 | loss: 0.4812  | val_0_auc: 0.73252 |  0:03:05s\n",
      "epoch 47 | loss: 0.48374 | val_0_auc: 0.73359 |  0:03:09s\n",
      "epoch 48 | loss: 0.479   | val_0_auc: 0.7323  |  0:03:13s\n",
      "epoch 49 | loss: 0.48133 | val_0_auc: 0.72647 |  0:03:17s\n",
      "Stop training because you reached max_epochs = 50 with best_epoch = 5 and best_val_0_auc = 0.76071\n",
      "    feature  importance\n",
      "18       18    0.284713\n",
      "7         7    0.091447\n",
      "1         1    0.086796\n",
      "12       12    0.072164\n",
      "20       20    0.066942\n",
      "4         4    0.059827\n",
      "34       34    0.053543\n",
      "25       25    0.051593\n",
      "2         2    0.038438\n",
      "5         5    0.024747\n",
      "21       21    0.019643\n",
      "19       19    0.018279\n",
      "16       16    0.016494\n",
      "32       32    0.013346\n",
      "22       22    0.012639\n",
      "30       30    0.010840\n",
      "8         8    0.010188\n",
      "35       35    0.008884\n",
      "17       17    0.007656\n",
      "9         9    0.007618\n",
      "28       28    0.007070\n",
      "11       11    0.006747\n",
      "3         3    0.004714\n",
      "26       26    0.004207\n",
      "29       29    0.004200\n",
      "33       33    0.004138\n",
      "23       23    0.002565\n",
      "13       13    0.002512\n",
      "6         6    0.002490\n",
      "31       31    0.002473\n",
      "0         0    0.000820\n",
      "27       27    0.000712\n",
      "10       10    0.000614\n",
      "24       24    0.000454\n",
      "14       14    0.000389\n",
      "15       15    0.000101\n",
      "Mean F1 Score (Macro F1 Score): 0.5782267376423156\n",
      "epoch 0  | loss: 0.64132 | val_0_auc: 0.71919 |  0:00:03s\n",
      "epoch 1  | loss: 0.59301 | val_0_auc: 0.73628 |  0:00:07s\n",
      "epoch 2  | loss: 0.5837  | val_0_auc: 0.74162 |  0:00:11s\n",
      "epoch 3  | loss: 0.57827 | val_0_auc: 0.73187 |  0:00:15s\n",
      "epoch 4  | loss: 0.57773 | val_0_auc: 0.74515 |  0:00:19s\n",
      "epoch 5  | loss: 0.57793 | val_0_auc: 0.73982 |  0:00:23s\n",
      "epoch 6  | loss: 0.56476 | val_0_auc: 0.74114 |  0:00:27s\n",
      "epoch 7  | loss: 0.56428 | val_0_auc: 0.74221 |  0:00:31s\n",
      "epoch 8  | loss: 0.56687 | val_0_auc: 0.73847 |  0:00:35s\n",
      "epoch 9  | loss: 0.55984 | val_0_auc: 0.74242 |  0:00:39s\n",
      "epoch 10 | loss: 0.5589  | val_0_auc: 0.74003 |  0:00:43s\n",
      "epoch 11 | loss: 0.55794 | val_0_auc: 0.7495  |  0:00:47s\n",
      "epoch 12 | loss: 0.55528 | val_0_auc: 0.74348 |  0:00:50s\n",
      "epoch 13 | loss: 0.55557 | val_0_auc: 0.74236 |  0:00:54s\n",
      "epoch 14 | loss: 0.55296 | val_0_auc: 0.73222 |  0:00:58s\n",
      "epoch 15 | loss: 0.55637 | val_0_auc: 0.73636 |  0:01:02s\n",
      "epoch 16 | loss: 0.5439  | val_0_auc: 0.73289 |  0:01:06s\n",
      "epoch 17 | loss: 0.54078 | val_0_auc: 0.72927 |  0:01:10s\n",
      "epoch 18 | loss: 0.54204 | val_0_auc: 0.73567 |  0:01:14s\n",
      "epoch 19 | loss: 0.53982 | val_0_auc: 0.72674 |  0:01:18s\n",
      "epoch 20 | loss: 0.53496 | val_0_auc: 0.72835 |  0:01:22s\n",
      "epoch 21 | loss: 0.53571 | val_0_auc: 0.7265  |  0:01:26s\n",
      "epoch 22 | loss: 0.5319  | val_0_auc: 0.71891 |  0:01:30s\n",
      "epoch 23 | loss: 0.52912 | val_0_auc: 0.73131 |  0:01:34s\n",
      "epoch 24 | loss: 0.52944 | val_0_auc: 0.71944 |  0:01:37s\n",
      "epoch 25 | loss: 0.53098 | val_0_auc: 0.7202  |  0:01:41s\n",
      "epoch 26 | loss: 0.52603 | val_0_auc: 0.72932 |  0:01:45s\n",
      "epoch 27 | loss: 0.51311 | val_0_auc: 0.73267 |  0:01:49s\n",
      "epoch 28 | loss: 0.5139  | val_0_auc: 0.71933 |  0:01:53s\n",
      "epoch 29 | loss: 0.51175 | val_0_auc: 0.72246 |  0:01:57s\n",
      "epoch 30 | loss: 0.50846 | val_0_auc: 0.72312 |  0:02:01s\n",
      "epoch 31 | loss: 0.50786 | val_0_auc: 0.72411 |  0:02:05s\n",
      "epoch 32 | loss: 0.50868 | val_0_auc: 0.72115 |  0:02:09s\n",
      "epoch 33 | loss: 0.50517 | val_0_auc: 0.7196  |  0:02:13s\n",
      "epoch 34 | loss: 0.49754 | val_0_auc: 0.71951 |  0:02:17s\n",
      "epoch 35 | loss: 0.49314 | val_0_auc: 0.7276  |  0:02:20s\n",
      "epoch 36 | loss: 0.4994  | val_0_auc: 0.71648 |  0:02:24s\n",
      "epoch 37 | loss: 0.49287 | val_0_auc: 0.71657 |  0:02:28s\n",
      "epoch 38 | loss: 0.49504 | val_0_auc: 0.72083 |  0:02:32s\n",
      "epoch 39 | loss: 0.49167 | val_0_auc: 0.72268 |  0:02:36s\n",
      "epoch 40 | loss: 0.48579 | val_0_auc: 0.71937 |  0:02:40s\n",
      "epoch 41 | loss: 0.48661 | val_0_auc: 0.718   |  0:02:44s\n",
      "epoch 42 | loss: 0.48706 | val_0_auc: 0.7211  |  0:02:48s\n",
      "epoch 43 | loss: 0.4818  | val_0_auc: 0.7149  |  0:02:52s\n",
      "epoch 44 | loss: 0.48397 | val_0_auc: 0.71273 |  0:02:56s\n",
      "epoch 45 | loss: 0.47946 | val_0_auc: 0.70909 |  0:03:00s\n",
      "epoch 46 | loss: 0.48292 | val_0_auc: 0.71387 |  0:03:04s\n",
      "epoch 47 | loss: 0.47688 | val_0_auc: 0.71768 |  0:03:08s\n",
      "epoch 48 | loss: 0.471   | val_0_auc: 0.71398 |  0:03:11s\n",
      "epoch 49 | loss: 0.4753  | val_0_auc: 0.7127  |  0:03:15s\n",
      "Stop training because you reached max_epochs = 50 with best_epoch = 11 and best_val_0_auc = 0.7495\n",
      "    feature  importance\n",
      "18       18    0.148752\n",
      "1         1    0.110088\n",
      "34       34    0.092230\n",
      "29       29    0.076544\n",
      "12       12    0.071653\n",
      "4         4    0.064569\n",
      "9         9    0.059580\n",
      "0         0    0.048063\n",
      "7         7    0.044580\n",
      "20       20    0.040835\n",
      "23       23    0.033971\n",
      "6         6    0.029363\n",
      "25       25    0.026740\n",
      "5         5    0.024758\n",
      "35       35    0.023196\n",
      "13       13    0.014693\n",
      "19       19    0.013381\n",
      "21       21    0.012039\n",
      "2         2    0.010354\n",
      "22       22    0.010000\n",
      "33       33    0.009618\n",
      "17       17    0.008954\n",
      "14       14    0.008873\n",
      "30       30    0.005417\n",
      "26       26    0.004468\n",
      "28       28    0.002148\n",
      "10       10    0.001826\n",
      "32       32    0.001272\n",
      "15       15    0.001086\n",
      "8         8    0.000610\n",
      "31       31    0.000235\n",
      "16       16    0.000035\n",
      "27       27    0.000030\n",
      "3         3    0.000028\n",
      "11       11    0.000008\n",
      "24       24    0.000003\n",
      "Mean F1 Score (Macro F1 Score): 0.5568661660334548\n",
      " \n",
      "Average F1 Score (Macro F1 Score): 0.5757283018841877\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import torch\n",
    "\n",
    "average_f1 = []\n",
    "predictions = []\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "f1_metric = F1Metric()\n",
    "\n",
    "for train_idx, val_idx in skf.split(X, y):\n",
    "    X_train, X_val = X.iloc[train_idx].values, X.iloc[val_idx].values\n",
    "    y_train, y_val = y.iloc[train_idx].values, y.iloc[val_idx].values\n",
    "    \n",
    "    # TabNetモデルの設定\n",
    "    model = TabNetClassifier(\n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        optimizer_params=dict(lr=2e-2),\n",
    "        scheduler_params={\"step_size\":50, \"gamma\":0.9},\n",
    "        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "        mask_type='entmax' # \"sparsemax\"もオプション\n",
    "    )\n",
    "\n",
    "    # モデルの訓練\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        max_epochs=50,\n",
    "        patience=100,\n",
    "        batch_size=256, \n",
    "        virtual_batch_size=128,\n",
    "        num_workers=0,\n",
    "        weights=class_weights,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    y_pred = model.predict(X_val)\n",
    "    mean_f1 = f1_score(y_val, y_pred, average='macro')\n",
    "    average_f1.append(mean_f1)\n",
    "\n",
    "    feature_importances_ = model.feature_importances_\n",
    "    feature = pd.DataFrame({'feature': X.columns, 'importance': feature_importances_})\n",
    "    feature.sort_values(by='importance', ascending=False, inplace=True)\n",
    "    print(feature)\n",
    "    print(\"Mean F1 Score (Macro F1 Score):\", mean_f1)\n",
    "\n",
    "average_f1 = np.mean(average_f1)\n",
    "print(' ')\n",
    "print(f'Average F1 Score (Macro F1 Score): {average_f1}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
